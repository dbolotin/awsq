#! /usr/bin/env python

import sys
import os
import argparse
from argparse import ArgumentError

import numpy as np
import errno
import string
import time
import random
import fcntl
import logging
import pickle
import select
import shlex
import base64
import watchtower

import shutil
import boto3
import itertools
import hashlib
import cPickle as pickle
import io
import zlib
from threading import Thread, Lock, Event

# importing subprocess or backport in python 2.x
from botocore.exceptions import ClientError

try:
    from Queue import Queue, Empty
except ImportError:
    from queue import Queue, Empty  # python 3.x

# if os.name == 'posix' and sys.version_info[0] < 3:
#     import subprocess32 as subprocess
# else:
import subprocess

logging.basicConfig(format="%(asctime)-15s %(name)s %(message)s")
common_logger = logging.getLogger("main")
common_logger.setLevel('INFO')


class Cmd:
    """ Represents command execution request. """

    def __init__(self, cmd, once_tag=None):
        self.cmd = cmd
        self.once_tag = once_tag

    def __repr__(self):
        if self.once_tag:
            return "Cmd{ %s ; once = %s }" % (self.cmd, self.once_tag)
        else:
            return "Cmd{ %s }" % self.cmd


class File:
    """ Represents job-message-embedded file. """

    def __init__(self, file_name, content):
        self.file_name = file_name
        self.content = content

    def __repr__(self):
        return "File{ %s, size=%s }" % (self.file_name, len(self.content))


class ExecutionEnvironment:
    def __init__(self, shared_folder, working_folder, log_group, log_stream_prefix, single_log_stream, session,
                 terminate_on_error):
        self.shared_folder = shared_folder
        self.working_folder = working_folder
        self.log_group = log_group
        self.log_stream_prefix = log_stream_prefix
        self.single_log_stream = single_log_stream
        self.session = session
        self.wt_handler = None
        self.single_logger = None
        self.terminate_on_error = terminate_on_error
        if log_group:
            self.wt_handler = watchtower.CloudWatchLogHandler(boto3_session=session, log_group=log_group)
            if self.single_log_stream:
                self.single_logger = logging.getLogger(self.single_log_stream)
                self.single_logger.setLevel('INFO')
                self.single_logger.addHandler(self.wt_handler)

    def create_logger(self, suffix):
        if self.single_logger:
            return self.single_logger
        logger = logging.getLogger(self.log_stream_prefix + suffix)
        logger.setLevel('INFO')
        if self.wt_handler:
            logger.addHandler(self.wt_handler)
        return logger

    def __repr__(self):
        items = ("%s = %r" % (k, v) for k, v in self.__dict__.items())
        return "<%s: {%s}>" % (self.__class__.__name__, ', '.join(items))


def atomic_only_file(filename):
    flags = os.O_CREAT | os.O_EXCL | os.O_WRONLY

    try:
        file_handle = os.open(filename, flags)
    except OSError as e:
        if e.errno == errno.EEXIST:
            return False
        else:  # Something unexpected went wrong so reraise the exception.
            raise
    else:  # No exception, so the file must have been created successfully.
        with os.fdopen(file_handle, 'w') as file_obj:
            file_obj.write("Lock")
        return True


def random_id(size=32, chars=string.ascii_uppercase + string.digits + string.ascii_lowercase):
    return ''.join(random.choice(chars) for _ in range(size))


def touch_open(filename, *args, **kwargs):
    # Open the file in R/W and create if it doesn't exist. *Don't* pass O_TRUNC
    fd = os.open(filename, os.O_RDWR | os.O_CREAT)

    # Encapsulate the low-level file descriptor in a python file object
    return os.fdopen(fd, *args, **kwargs)


ON_POSIX = 'posix' in sys.builtin_module_names


class CmdExecutor:
    def __init__(self, cmd, env, working_folder, job, environment, logger):
        self.cmd = cmd
        self.env = env
        self.working_folder = working_folder
        self.job = job
        self.proc = None
        self.environment = environment
        self.logger = logger
        # self.mutex = Lock()
        # self.output_buffer = []

    def catch_output(self, stream, is_err):
        for line in iter(stream.readline, b''):
            if is_err:
                self.logger.info("ERR: " + line[:-1])
            else:
                self.logger.info("OUT: " + line[:-1])
        stream.close()

    # def process_output(self):
    #     with self.mutex:
    #         buf = list(self.output_buffer)
    #         self.output_buffer = []
    #     while self.proc.returncode is None or len(buf) > 0:
    #         for line in buf:
    #             self.logger.info(line)
    #         time.sleep(2)
    #         with self.mutex:
    #             buf = list(self.output_buffer)
    #             self.output_buffer = []
    #     self.logger.info("Finished processing output.")

    def execute(self):
        self.logger.info("Executing.")
        self.proc = subprocess.Popen(self.cmd.cmd, cwd=self.working_folder, env=self.env,
                                     shell=True, bufsize=0, stdout=subprocess.PIPE,
                                     stderr=subprocess.PIPE,
                                     close_fds=ON_POSIX)
        t_out = Thread(target=self.catch_output, args=(self.proc.stdout, False))
        t_out.start()
        t_err = Thread(target=self.catch_output, args=(self.proc.stderr, True))
        t_err.start()
        # t_process = Thread(target=self.process_output, args=())
        # t_process.start()

        self.proc.wait()

        t_out.join()
        t_err.join()

        self.logger.info("Done. Exit code = %s", self.proc.returncode)

        return self.proc.returncode == 0


class Job:
    def __init__(self, variables, commands, log_group=None, job_id=None, log_stream=None):
        if not job_id:
            job_id = random_id()
        self.job_id = job_id
        self.variables = variables
        self.commands = commands
        self.output_buffer = []
        self.log_group = log_group
        self.log_stream = log_stream

    def execute(self, environment):
        logger = environment.create_logger(self.job_id)

        # Generate folder name
        folder = os.path.join(environment.working_folder, random_id())

        # Check that if folder not exists
        assert not os.path.exists(folder)

        # Creating folder
        os.makedirs(folder)

        try:
            # Creating evn (environment variables objects)
            env = os.environ.copy()
            env["AWSQ_SHARED_FOLDER"] = environment.shared_folder
            for var, value in self.variables.iteritems():
                if not isinstance(value, str):
                    value = str(value)
                env[var] = value

            logger.info("Variables: %s", self.variables)

            # Executing commands
            for step in self.commands:
                logger.info("Executing: %s.", step)
                # File
                if isinstance(step, File):
                    logger.info("Writing file.")
                    with open(os.path.join(folder, step.file_name), 'w') as f:
                        f.write(step.content)

                # Cmd
                elif isinstance(step, Cmd):
                    executor = CmdExecutor(step, env, folder, self, environment, logger)
                    if not step.once_tag:
                        if not executor.execute():
                            return False
                    else:
                        logger.info("Acquiring system-wide lock %s.", step.once_tag)
                        lock_file = os.path.join(environment.shared_folder, step.once_tag + ".lock")
                        # Open an existing file or create if it doesn't exist
                        with touch_open(lock_file, "r+") as fd:
                            # Acquire a exclusive lock
                            fcntl.lockf(fd, fcntl.LOCK_EX)

                            # Read a previous value if present
                            previous_value = fd.read()
                            if previous_value != "Done":
                                if executor.execute():
                                    # Write the new value and truncate
                                    fd.seek(0)
                                    fd.write("Done")
                                    fd.truncate()
                                else:
                                    return False
                            else:
                                logger.info("Action already executed.")
        except:
            return False
        finally:
            if not environment.terminate_on_error:
                shutil.rmtree(folder)

        return True


# Arguments parser
#

# Python issue 27859 example workaround
class FixParserAction(argparse._SubParsersAction):
    def __call__(self, parser, namespace, values, option_string=None):
        parser_name = values[0]
        arg_strings = values[1:]

        # set the parser name if requested
        if self.dest is not argparse.SUPPRESS:
            setattr(namespace, self.dest, parser_name)

        # select the parser
        try:
            parser = self._name_parser_map[parser_name]
        except KeyError:
            args = {'parser_name': parser_name,
                    'choices': ', '.join(self._name_parser_map)}
            msg = _('unknown parser %(parser_name)r (choices: %(choices)s)') % args
            raise argparse.ArgumentError(self, msg)

        # parse all the remaining options into the namespace
        # store any unrecognized options on the object, so that the top
        # level parser can decide what to do with them

        # ORIGINAL
        namespace, arg_strings = parser.parse_known_args(arg_strings, namespace)

        # issue 9351 change
        ## In case this subparser defines new defaults, we parse them
        ## in a new namespace object and then update the original
        ## namespace for the relevant parts.
        # subnamespace, arg_strings = parser.parse_known_args(arg_strings, None)
        # for key, value in vars(subnamespace).items():
        #    setattr(namespace, key, value)

        if arg_strings:
            vars(namespace).setdefault(argparse._UNRECOGNIZED_ARGS_ATTR, [])
            getattr(namespace, argparse._UNRECOGNIZED_ARGS_ATTR).extend(arg_strings)


class QNamespace:
    """ Used to parse CLI arguments """

    def __init__(self):
        self.profile = None
        self.queue = None
        self.once = False
        self.region = None
        self.shared_dir = '.'
        self.working_dir = '.'
        self.commands = []
        self.variables_generator = [{}]
        self.idle_time = None
        self.subcommand = None
        self.log_group = None
        self.log_stream_prefix = ""
        self.single_log_stream = None
        self.terminate_on_error = False

    def get_execution_environment(self, session):
        return ExecutionEnvironment(os.path.abspath(self.shared_dir),
                                    os.path.abspath(self.working_dir),
                                    self.log_group, self.log_stream_prefix,
                                    self.single_log_stream, session, self.terminate_on_error)

    def reset_once(self):
        self.once = False

    def add_cmd(self, cmd, ignore_once=False):
        if ignore_once:
            self.commands.append(Cmd(cmd))
        else:
            self.commands.append(Cmd(cmd, self.get_once_tag(cmd)))
            self.reset_once()

    def add_file(self, l_file_name, r_file_name=None):
        if not r_file_name:
            r_file_name = l_file_name
        size = os.stat(l_file_name).st_size
        if size > 200000:
            raise argparse.ArgumentError(self, "File %s is too big to be "
                                               "attached to the job." % l_file_name)
        with open(l_file_name, 'r') as f:
            self.commands.append(File(r_file_name, f.read()))

    def set_once(self, once_tag):
        if once_tag:
            self.once = once_tag
        else:
            self.once = True

    def get_once_tag(self, content):
        once_tag = None
        if self.once:
            if self.once == True:
                once_tag = hashlib.md5(content).hexdigest()
            else:
                once_tag = self.once
        return once_tag


class OnceParserAction(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        namespace.set_once(values)


class CmdParserAction(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        namespace.add_cmd(values)


def variable_generator(dict):
    # Extracting variable / values form dict
    var_names = []
    var_values = []
    for key, value in dict.iteritems():
        var_names.append(key)
        var_values.append(value)

    # Output map function
    def to_dict(value_list):
        result = {}
        for i in range(len(value_list)):
            result[var_names[i]] = value_list[i]
        return result

    # Returning result
    return map(to_dict, itertools.product(*var_values))


class VariableParserAction(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        if not values:
            return

        generators = []

        variables = {}
        for v in values:
            # New variable group start
            if v == '::':
                generators.append(variable_generator(variables))
                variables = {}
                continue

            eq_index = v.find('=')
            if eq_index == -1:
                raise ArgumentError(self, 'Variable "%s" contain no value.' % v)

            name = v[:eq_index]
            value = v[eq_index + 1:]

            if value == "-":
                value = "@map(lambda line: line[:-1], sys.stdin)"

            if value.startswith("@"):
                value = eval(value[1:])
            else:
                value = [value]

            variables[name] = value

        if variables:
            generators.append(variable_generator(variables))

        namespace.variables_generator = itertools.chain(*generators)


class FileParserAction(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        l_file_name = values
        r_file_name = values
        if ":" in l_file_name:
            spl = l_file_name.split(":")
            l_file_name = spl[0]
            r_file_name = spl[1]
        namespace.add_file(l_file_name, r_file_name)


class ExecuteParserAction(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        namespace.add_file(values)
        namespace.add_cmd("chmod +x " + values, ignore_once=True)
        namespace.add_cmd("./" + values)


class MessageKeeper(Thread):
    def __init__(self, queue_url, receipt_handle, timeout=120):
        Thread.__init__(self)
        self.queue_url = queue_url
        self.receipt_handle = receipt_handle
        self.timeout = timeout
        self.ended = Event()

    def reset_visibility_timeout(self):
        sqs_client.change_message_visibility(
            QueueUrl=self.queue_url,
            ReceiptHandle=self.receipt_handle,
            VisibilityTimeout=self.timeout
        )

    def run(self):
        self.reset_visibility_timeout()
        # 10 seconds for network delays etc..
        while not self.ended.wait(timeout=self.timeout - 10):
            self.reset_visibility_timeout()


# Creating arg parser

# Create the top-level parser
main_parser = argparse.ArgumentParser(prog='awsq')

# chg which Action class is used for subparsers
main_parser.register('action', 'parsers', FixParserAction)

main_parser.add_argument('--queue', help='SQS queue name')
main_parser.add_argument('--region', help='AWS region of the queue')
main_parser.add_argument('--profile', help='AWS Credentials profile')
subparsers = main_parser.add_subparsers(dest='subcommand', help='sub-command help')

# "execute" subcommand
parser_execute = subparsers.add_parser('execute', help='executes command from the queue (must be started on '
                                                       'execution node)')
parser_execute.add_argument('--idle-time', dest='idle_time', type=int,
                            help='If specified, after this number of seconds in idle state (no messages) '
                                 'this process will be terminated.')
parser_execute.add_argument('--shared-dir', dest='shared_dir', default='.',
                            help='Shared folder across several execution instances. Used to create lock files, '
                                 'and store shared resources. Persists across separate job executions.')
parser_execute.add_argument('--working-dir', dest='working_dir', default='.',
                            help='Folder to create individual job execution folders.')
parser_execute.add_argument('--single-log-stream', metavar="STREAM_NAME", dest='single_log_stream',
                            help='Post all log messages to the same CloudWatch Log Stream. If this option is '
                                 'specified --log-stream-prefix will be ignored.')
parser_execute.add_argument('--log-group', metavar="GROUP_NAME", dest='log_group',
                            help='Name of CloudWatch Log Group to log into')
parser_execute.add_argument('--log-stream-prefix', metavar="STREAM_NAME_PREFIX", dest='log_stream_prefix',
                            help='CloudWatch Log stream prefix. Ignored if --single-log-stream specified.')
parser_execute.add_argument('--terminate-on-error', dest='terminate_on_error', action='store_true',
                            help='CloudWatch Log stream prefix. Ignored if --single-log-stream specified.')

# "submit" subcommand
parser_submit = subparsers.add_parser('submit', help='pushes new command to queue')
parser_submit.add_argument('--cmd', help='Command', action=CmdParserAction)
parser_submit.add_argument('--attach',
                           help='Attach small file to job (e.g. script, config file, etc...). '
                                'File will be placed to working directory. To specify other '
                                'remote filename of the file use colon symbol, e.g. '
                                '"local_file_name.txt:remote_file.cfg").',
                           action=FileParserAction)
parser_submit.add_argument('--execute', help='Attach executable file to job and execute it on execution node.',
                           action=ExecuteParserAction)
parser_submit.add_argument('--once', nargs='?', help='Command', action=OnceParserAction)

parser_submit.add_argument('vars', metavar='VAR=value', nargs='*',
                           help='Variable value / values. Will be parsed using python\'s '
                                'eval(...) function.',
                           action=VariableParserAction)

# Parsing args
args = main_parser.parse_args(namespace=QNamespace())

# Creating boto3 clients
boto_session = boto3.Session(region_name=args.region, profile_name=args.profile)
sqs_client = boto_session.client('sqs')

# Getting queue URL
try:
    response = sqs_client.get_queue_url(QueueName=args.queue)
    queue_url = response["QueueUrl"]
except ClientError as exec_env:
    if "NonExistentQueue" in exec_env.response["Error"]["Code"]:
        print("No queue with name: " + args.queue)
    else:
        print("Error while retrieving queue url.")
        print(exec_env.response)
    exit(1)

if args.subcommand == "submit":
    batch = []
    for c in args.variables_generator:
        job = Job(c, args.commands)
        serialized_job = io.BytesIO()
        pickle.Pickler(serialized_job).dump(job)
        # compressed_job = zlib.compress(serialized_job.getvalue(), 9)
        compressed_job = base64.standard_b64encode(zlib.compress(serialized_job.getvalue(), 9))

        batch.append({"Id": str(len(batch)), "MessageBody": compressed_job})

        if len(batch) == 10:
            print("Submitting jobs.")
            result = sqs_client.send_message_batch(QueueUrl=queue_url, Entries=batch)
            if "Failed" in result:
                print("Error")
                print(result)
                exit(1)
            common_logger.info("Submitted %s.", len(batch))
            batch = []

    if len(batch) > 0:
        result = sqs_client.send_message_batch(QueueUrl=queue_url, Entries=batch)
        if "Failed" in result:
            print("Error")
            print(result)
            exit(1)
        common_logger.info("Submitted %s.", len(batch))

elif args.subcommand == "execute":
    exec_env = args.get_execution_environment(boto_session)
    common_logger = exec_env.create_logger("main")
    common_logger.info("Execution environment: %s.", exec_env)
    last_good_msg = int(round(time.time() * 1000))
    common_logger.info("Starting polling for messages.")
    while True:
        resp = sqs_client.receive_message(QueueUrl=queue_url, MaxNumberOfMessages=1, VisibilityTimeout=10,
                                          WaitTimeSeconds=20)

        current_time = int(round(time.time() * 1000))

        if "Messages" in resp:
            last_good_msg = current_time
            msg_stream = io.BytesIO(zlib.decompress(base64.standard_b64decode(resp["Messages"][0]["Body"])))
            receipt_handle = resp["Messages"][0]["ReceiptHandle"]
            job = pickle.Unpickler(msg_stream).load()
            keeper = MessageKeeper(queue_url, receipt_handle)
            keeper.start()
            result = job.execute(exec_env)
            keeper.ended.set()
            if result:
                resp = sqs_client.delete_message(
                    QueueUrl=queue_url,
                    ReceiptHandle=receipt_handle
                )
            else:
                common_logger.info("Error.")
                if exec_env.terminate_on_error:
                    common_logger.info("Terminating.")
                    exit(1)

        if args.idle_time is not None and current_time - last_good_msg > args.idle_time * 1000:
            common_logger.info("Idle limit exceeded.")
            exit(0)

        common_logger.info("Idle %s.", current_time - last_good_msg)
